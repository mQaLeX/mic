# BYTEWEIGHT

控制流完整性CFI

At a high level, we learn signatures for function starts using a weighted prefix tree, and recognize function starts by matching binary fragments with the signatures. Each node in the tree corresponds to either a byte or an instruction, with the path from the root node to any given node representing a possible sequence of bytes or instructions. The weights, which can be learned with a single linear pass over the data set, express the confidence that a sequence of bytes or instructions corresponds to a function start. After function start identification, we then use value set analysis (VSA) [2] with an incremental control flow recovery algorithm to find function bodies with instructions, and extract function boundaries.

在高层次上，我们使用加权前缀树学习函数开始的签名，并通过将二进制片段与签名匹配来识别函数开始。树中的每个节点对应一个字节或一条指令，从根节点到任何给定节点的路径代表可能的字节或指令序列。权重可以通过对数据集的单次线性传递来学习，表达了字节或指令序列对应于函数开始的置信度。在函数启动识别之后，我们然后使用值集分析（VSA）[2]和增量控制流恢复算法来查找带有指令的函数体，并提取函数边界。

## 定义

可执行代码是一个二进制串$B$, 规定$B[i]$代表该二进制串的第$i$个字节, $B[i:i+j]$代表该串中$B[i], B[i+1], ..., B[i+j-1]$这$j$个字节构成的字串. 我们假设段偏移为0, 即$B[i]$字节在内存中的地址为$i$.

**函数**$F$是一个字节集合, 即
$$
F = \{B[i], B[j], ..., B[n]\}
$$
定义二进制串$B$的**函数集**$\text {FUNCS}(B)$​为二进制串中所有的函数, 即:
$$
\mathbf {FUNCS}(B) = \{F_1, F_2, ..., F_k\}
$$
**函数**$F_i$的**起始地址**$s_i$为函数中字节的最小地址, 即
$$
s_i = \text{minaddr}(F_i)
$$

**函数**$F_i$的**结束地址**$e_i$为函数中字节的最大地址, 即
$$
e_i = \text{maxaddr}(F_i)
$$

**函数**$F_i$的**边界**$(s_i, e_i)$​为上述二者的简单组合.

**函数起始识别 **(**Function Start Identification**) (**FSI**) 问题: 给定由包含$k$个函数的源文件编译得到的二进制串$B$, 输出函数起始地址集合$\{s_1, s_2, ..., s_k\}$

**函数边界识别** (**Function Boundary Identification**) (**FBI**) 问题: 给定由包含$k$​个函数的源文件编译得到的二进制串$B$​, 输出函数边界集合$\{(s_1, e_1), (s_2, e_2), ..., (s_k, e_k)\}$

**函数识别** (**Function Identification**) (**FI**) 问题: 给定由包含$k$个函数的源文件编译得到的二进制串$B$, 输出函数集合$\{F_1, F_2, ..., F_k\}$​

**函数预言机** (**Function Oracle**)
$$
\mathbf{O}_{\text{func}}(B) = \mathbf {FUNCS}(B)
$$
**(函数)边界预言机** (**Boundary Oracle**)
$$
\mathbf{O}_{\text{bound}}(B) = \{(s_1, e_1), (s_2, e_2), ..., (s_k, e_k)\}
$$
**(函数)起始预言机** (**Start Oracle**)
$$
\mathbf{O}_{\text{start}}(B) = \{s_1, s_2, ..., s_k\}
$$
对于FSI问题, 设输出的函数起始地址集合为$S = \{s_1, s_2, ..., s_k\}$​​, 定义:

真阳性集合TP为正确识别的函数起始地址的集合, 即:
$$
\text{TP} = S \cap \mathbf{O}_{\text{start}}(B)
$$
假阳性集合FP为识别出的本不存在的函数起始地址的集合, 即:
$$
\text{FP} = S \setminus \mathbf{O}_{\text{start}}(B)
$$
假阴性集合FN为未识别出的但实际存在的函数起始地址的集合, 即:
$$
\text{FN} = \mathbf{O}_{\text{start}}(B) \setminus S
$$
定义**预测精度** (**Precision**)
$$
\text {Precision} = \frac{\abs{\text{TP}}}{\abs{\text{TP}} + \abs{\text{FP}}}
$$
定义**召回率 **(**Recall**)
$$
\text {Recall} = \frac{\abs{\text{TP}}}{\abs{\text{TP}} + \abs{\text{FN}}}
$$
定义 **F1**
$$
\text{F1} = \frac{2 \times \text {Precision} \times \text {Recall}}{\text {Precision} + \text {Recall}}
$$


只考虑可执行代码, 

可执行代码是一个二进制字符串

函数是一个字节集合, 字节不必连续, 函数的最低地址称为函数起始地址, 函数的结束地址为函数的最高地址, 函数边界是函数的起始地址和结束地址对

二进制文件的函数集是该文件中所有函数的集合, 任意两个函数的交集不一定为空

函数预言机, 边界预言机, 开始预言机

函数识别(FI)算法, 函数边界识别(FBI)算法, 函数开始(FSI)识别算法

## 挑战

because of: 优化编译器

- (由于对齐和填充), 有些字节不属于任何一个函数: 

  假设我们有二进制 B 的符号表信息。一个简单的启发式算法是首先按地址对符号表条目进行排序，然后将条目 fi 和 fi+1 之间的每个字节归为函数 fi。(有缺陷)

- 函数不一定是连续的:

  间隙中可能有跳表, 数据, 或其他函数的指令

  函数共享代码也会造成函数不连续

- 函数可能无法到达

  从程序开始的递归式反汇编算法, 将错过未被调用的函数. (有缺陷)

  造成不可达性的原因有编译器的优化, (常量计算)

- 函数可能有多个入口点

- 函数可能被内联化

  因此即使有源码, 二进制分析也是必要的

- 二进制代码受到编译器, 编译器版本和特定优化的严重影响

## ByteWeight概述

FSI问题视为分类问题, 每个字节标记为是否为函数开始

两种形式: 使用原始字节的形式和使用规范化反汇编指令的形式

- 训练阶段: 编译源代码, 生成已知起始地址的二进制文件. 在高层次上，我们的算法创建了一个已知函数起始字节或指令序列的加权前缀树。我们通过计算参考数据集中每个序列的真阳性与真假阳性总和的比率来对前缀树中的顶点进行加权。

- 分类阶段，我们使用加权前缀树来确定给定的字节或指令序列是否对应于函数开始。 如果前缀树中相应的终端节点的权重值大于阈值 t，我们就说一个序列对应于一个函数开始。 在序列与前缀树中的路径完全匹配的情况下，终端节点是该路径中的最后一个节点。 如果序列与树中的路径不完全匹配，则终端节点是序列中最后匹配的节点。

## 具体步骤

1. 训练阶段, 输出加权前缀树

   1. 以各种优化级别编译源代码, 保留调试信息

   2. 提取每个函数的前l个元素(字节或指令)

   3. 生成前缀树, 两种规范化(立即数规范化, 调用和跳转指令规范化)

      If the sequence is over bytes, the prefix tree is calculated directly, although our experiments indicate that a prefix tree calculated over normalized instructions fairs better.(不懂)

   4. 计算权值, 匹配和假匹配

   5. 前缀树节点修剪, 基于事实: 1. 若父节点没有假匹配, 则子节点也没有假匹配. 2. 具有相同权重的子节点是冗余的

2. 分类阶段, 输入二进制文件, 加权前缀树, 权重阈值

   1. 对输入的二进制文件与树匹配, 对于每次对树的遍历, 当最后一个匹配节点的权重不小于权重阈值时, 我们说该序列是函数的开头

## FI问题

解决FSI问题后, 从函数开始进行CFG恢复

使用递归反汇编跟踪直接控制转移(直接跳转和直接调用), 使用**值集分析**(VSA)跟踪间接控制转移(间接调用和跳转表), 最终生成函数的控制流图(CFG)

维护一个数组, 每一轮, 找到下一条可达指令, 如果不在数组里, 则加入它.

(这里把call指令跳转到的另一个函数也算作该函数的一部分了)(If it is a call mnemonic, the reachable instructions include both the call reference and the instruction directly following the call instruction.)

有一些无返回函数(例如`abort`和`exit`)需要注意.

## RFCR技术

recursive function call resolution(递归函数调用解析)(太会起名了)

在CFG恢复期间继续补充函数开始列表, 对于call指令的目的地址, 认为它是函数的开始, 如果它不在函数开始列表里, 则加入它.

## 实现

使用BAP构建CFG, objdump提取符号表, Dia2dump解析PDB文件(似乎是windows下的)

## 相关工作

In addition to the already discussed Rosenblum et al. [27], there are a variety of existing binary analysis platforms tackle the binary identification problem. BitBlaze [6] assumes debug information. If no debug information is present, it treats the entire section as one function. Bit- Blaze also provides an interface for incorporating Hex Rays function identification information.

Dyninst [19] also offers tools, such as unstrip [31], to identify functions in binaries without debug information. Within the Dyninst framework, potential functions in the .text section are identified using the hex pattern 0x55 representing push %ebp. First, Dyninst will start at the entry address and traverse inter- and intra-procedural control flow. The algorithm will scan the gaps between functions and check if push %ebp is present. This does not preform well across different optimizations and operating systems.

IDA using proprietary heuristics and FLIRT [16] technique attempts to help security researchers recover procedural abstractions. However, updating the signature database requires an amount of manual effort that does not scale. In addition, because FLIRT uses a pattern matching algorithm to search for signatures, small variations in libraries such as different compiler optimizations or the use of different compiler versions, prevent FLIRT from recognizing important functions in a disassembled program. The Binary Analysis Platform (BAP) also attempts to provide a reliable identification of functions using custom-written signatures [8].


Kruegel et al. perform exhaustive disassembly, then use unigram and bigram instruction models, along with patterns, to identify functions [23]. Jakstab uses two predefined patterns to identify functions for x86 code [22, §6.2].







# Recognizing Functions in Binaries with Neural Networks

## 绪论

在许多二进制分析技术中, **函数识别**都是第一步, 本文使用**循环神经网络**完成这一工作, 比最先进的基于机器学习的方法要更准确和高效, 体现在模型训练速度提高(一个数量级), 函数识别速度提高(几百倍), 错误率降低(6/8**benchmark**下减半, 其余benchmark下相当).

## 介绍

在源码常常不可用的情况下, 二进制分析技术驱动了很多计算机安全应用, 例如恶意软件检测, 漏洞修复, 协议逆向.

函数是程序中重要的结构块, 但二进制文件是机器指令串. 依赖于**函数边界**信息的二进制分析技术首先要尝试通过函数识别来回复函数结构.

*Byteweight: Learning to recognize functions in binary code*指出, IDA Pro和 the CMU Binary Analysis Platform等流行工具使用的简单函数识别技术准确性较差. Byteweight方法基于机器学习, 准确性有提升, 但仍有改进空间.

本文使用神经网络, 优点: 首先，他们可以直接从**原始表示**中学习，只需要最少的**预处理**(they can learn directly from the original representation with minimal preprocessing (or “**feature engineering**”) needed.), 其次，神经网络可以**端到端**地学习，其中每个组成阶段都同时训练，以便最好地解决最终目标。(Second, neural networks can learn end-toend, where each of its constituent stages are trained simultaneously in order to best solve the end goal. )

之前没有人用神经网络做过二进制分析方面的作品. 但本文实验结果表明它能够有效解决函数识别任务.

本文训练一个循环神经网络, 以二进制字节流作为输入, 对于每个位置, 预测该位置是否存在函数边界.

## 定义

### 代码和函数

本文抛弃二进制可执行文件中的其他部分, 关注机器代码.

将代码和函数看成字节集而非指令集

代码$C$是一个字节序列$C[1], C[2], ..., C[l]$, $C[i]$是一个字节

代码中的$n$个函数是$f_1, f_2, ..., f_n$

函数$f_i$的字节索引序列是$f_{i,1}, f_{i,2}, ..., f_{i,l_i}$, $l_i$是$f_i$的总字节数

**每个字节可能属于任意数量的函数**，函数可以包含任何字节集，无论是否连续。

### 任务

假设可以访问二进制代码$C$, 但没有关于任意函数$f_i$的信息, 任务:

- 识别函数开始位置: 即给定$C$, 找到$\{f_{1,1}, f_{2,1}, ..., f_{n,1}\}$
- 识别函数结束位置: 即给定$C$, 找到$\{f_{1,l_1}, f_{2,l_2}, ..., f_{n,l_n}\}$
- 识别函数边界: 即给定$C$, 找到$\{(f_{1,1}, f_{1,l_1}), (f_{2,1}, f_{2,l_2}), ..., (f_{n,1}, f_{n,l_n})\}$, 又即在上述两个任务的基础上将开始和结束位置配对
- 识别函数: 即给定$C$, 找到$\{(f_{1,1}, ..., f_{1,l_1}), (f_{2,1}, ..., f_{2,l_2}), ..., (f_{n,1}, ..., f_{n,l_n})\}$

本文解决前三个问题, 第四个问题未解决(WTF?)

### 评价

$TP$: 真阳性预测的数量, $FP$: 假阳性预测的数量, $FN$: 假阴性预测的数量

精度(precision) = $\frac{TP}{TP + FP}$

召回率(机翻)(recall) = $\frac{TP}{TP+FN}$

F1 = $\frac{2 \times precision \times recall}{precision + recall}$

优点: 由于程序中的大多数字节都不会开始或结束函数，因此与简单的准确度指标相比，这些指标可以更好地说明模型的有效性。 例如，预测代码中不存在函数将提供超过 99.9% 的准确度，因为少于 0.1% 的字节开始或结束函数。 准确度指标并未表明这些预测大多无用。 相比之下，这些预测将实现的 0% 召回率清楚地表明。(机翻)

### 启发性方法的局限性

O3优化后的代码很奇怪

## 神经网络

多层感知机(MLP)与损失函数

循环神经网络(RNN), 双向, 多层

长短程存储(LSTM)

门控循环单元(GRU)

## 实现

使用one-hot的256位向量来表示一个字节

反转字节顺序

## 错误分析

• 鉴于模型的双向性，除了典型的函数序言外，它似乎还利用了在前一个函数末尾频繁出现的序列的出现。一个明显的例子是 ret 及其变体，用于从函数执行中返回。编译器还经常在函数之间插入填充（例如 nop (0x90) 和其他具有更长编码的无操作指令，或在 Windows 二进制文件中，触发中断的 int3），模型将使用其结尾来识别职能。

• 由于上述原因，在 nop、ret 和其他通常出现在函数末尾的指令之后经常会出现误报。事实上，如果它们包含 0x90 或 0xc3（这些指令的编码），它也会在编码到代码中的立即值中发现误报。

• 如我们所料，当通常出现在函数中间的指令出现在函数的开头时，经常会出现误报。程序的第一个字节通常也被错误地识别为函数开始，这可能是由于缺少之前的上下文。

• 如 Bao 等人所记载。 [2]，icc 将生成具有多个入口点的函数。许多误报发生在函数的第二个入口点，因为它之前的指令不是通常结束函数的指令。

• 模型的行为不容易通过短指令序列的简单规则来表征；例如，虽然在 nop 和 ret 之后发生了许多误报，但这并不意味着模型将所有（甚至大部分）此类位置标记为函数启动。对于此类相对困难的情况，周围字节的精确内容可能会对模型生成的答案产生复杂的影响。

• Given the bidirectionality of the model, it seems to exploit the appearance of frequently-occurring sequences at the ends of the previous function in addition to typical function prologues. One obvious example are ret and its variants, used to return from function execution. The compiler also often inserted padding between functions (such as nop (0x90) and other no-op instructions with longer encodings, or in Windows binaries, int3 which triggers an interrupt), the end of which the model would use to recognize the beginnings of functions. 

• As a consequence of the above, false positives often occurred after nop, ret, and other instructions which usually appear at the end of a function. In fact, it would also find false positives within immediate values encoded into the code if they contained 0x90 or 0xc3, the encodings of those instructions.

• False negatives often occurred when instructions that would typically occur in the middle of functions occurred at the beginning of a function, as we might expect. The first byte of the program was often also falsely not recognized as a function start, presumably due to the lack of context previous to it.

• As documented by Bao et al. [2], icc will generate functions with multiple entry points. Many of the false negatives occurred at the second entry points to functions, given that the instructions before it are not the ones which usually end functions.

• The behavior of the model was not easily characterized by simple rules on short sequences of instructions; for example, while many false positives occurred after nop and ret, this did not mean that the model marked all (or even a large fraction) of such positions as function starts. For relatively difficult cases like these, the precise content of the surrounding bytes might have a complicated effect on the answer produced by the model.

# Semantics-Aware Machine Learning for Function Recognition in Binary Code

## 介绍

基于语义信息识别函数

基于语法的方法无法对抗混淆

**符号执行**捕获程序的语义信息。符号执行的关键思想是使用符号变量来表示输入并（静态地）解释代码。符号执行后，对于每个初始输入，生成一个符号公式来表示其输出语义。

用符号执行和机器学习的结合来识别函数

1. 使用Uroboros反汇编输入二进制文件并恢复基本块
2. 在每个基本块上应用符号执行来生成相应的语义(记录每个寄存器的赋值公式, 访存)
3. 从符号执行的输出中选择关键语义, 转换为特征向量, 通过机器学习训练模型
4. 对于任何给定的基本块, 模型回答它是否代表一个函数入口点基本块

使用Obfuscator-LLVM测试

## 例子

加入一个nop指令就能让Byte-Weight失效

## 概述

通过函数入口点基本块的语义来训练分类器

FID方法建立在Uroboros上, Uroboros是一个开源反汇编平台, 本文用许多分析函数扩展了Uroboros.

假设反汇编和基本块恢复是可靠的.

FID几乎没有考虑程序语法, 除了机器学习之外也使用了call指令的目的地址

### 工作流程

将stripped二进制文件作为输入

使用Uroboros恢复程序控制流结构(反汇编, 基本块), 简化指令

启动符号执行引擎生成每个基本块的语义(寄存器公式和访存行为), 选取重要语义

将语义转换为特征向量

训练: 为每种编译器生成一个模型

分类: 先预分类, 确定该二进制文件是从哪个编译器编译的(?), 然后用对应编译器的模型分类

设计策略是: 保守的特征选择以保证高精度(减少假阳数量), 使用调用指令收集机制(call instruction collection)机制来提高召回率

### 基本块上的符号执行

在每个基本块上, 对每个寄存器生成赋值公式, 生成内存读取公式.

### 选择重要语义

栈指针 ebp, esp的赋值公式

栈内存读取

### 语义转换为数值向量

提取词法和句法特征, 每个赋值公式提取8个特征, 提取ebp和esp的赋值公式, 还根据栈内存访问行为捕获了三个布尔 (0/1) 堆栈特征。因此每个基本块会转换为19维的特征向量

1. 词法特征
   | 特征               | 定义                                                       |
   | ------------------ | ---------------------------------------------------------- |
   | numOperator/length | 赋值公式中运算符的出现次数除以公式的总长度(以字符计, 下同) |
   | numToken/length    | 赋值公式中Token的数量除以公式的总长度                      |
   | numConstant/length | 赋值公式中常量的数量除以公式的总长度                       |
   | decOperator/length | 赋值公式中减法运算符的数量除以公式的总长度                 |
   | decNum/length      | 赋值公式中小操作数的减法运算符的数量除以公式的总长度       |

2. 句法(Syntactic)特征

   | 特征            | 定义                                               |
   | --------------- | -------------------------------------------------- |
   | maxNestingDepth | 赋值公式中嵌套括号的最大深度                       |
   | maxDepthASTNode | 赋值公式转换成的AST(抽象语法树)的最大深度          |
   | aveTreeDistance | 目标AST和50个随机选取的AST之间的树编辑距离的平均值 |

3. 栈特征(bool值)(3个)

    | 特征        | 定义                                                         |
    | ----------- | ------------------------------------------------------------ |
    | memREAD$_n$ | 是否在(reg8 + 4 * n)的内存位置上有读取操作(参数), n = 1, 2, 3 |

上述特征的提取应用了规范化思想, 即对公式进行概括, 以便它可以匹配具有相似结构的公式, 在之前的基于语法的方法和本文基于语义的方法中都用到了这个思想.

### 预分类-区分不同的编译器

gcc和icc编译出来的代码语义特征相似, LLVM编译出来的代码与二者有差异

预分类确定该二进制程序使用的编译器, 之后使用对应编译器的模型来识别函数

预分类第一步从所有基本块中选取关键基本块, 即对应于函数入口点的基本块, 再即call指令的目标地址所对应的基本块;

第二步提取这些关键基本块的五个布尔特征:

| 特征    | 定义                |
| ------- | ------------------- |
| decESP  | esp是否包含减法运算 |
| decEBP  | ebp是否包含减法运算 |
| memREAD | 同上                |

布尔特征使得向量分布空间小(32), 训练多数投票分类器模型



# TREX: Learning Execution Semantics from Micro-Traces for Binary Similarity
## 摘要

Detecting semantically similar functions – a crucial analysis
capability with broad real-world security usages including
vulnerability detection, malware lineage, and forensics – requires
understanding function behaviors and intentions.

检测语义相似的函数-一项具有广泛的现实安全用途的重要分析能力，包括漏洞检测，恶意软件血统和取证-需要了解函数行为和意图。

However, this task is challenging as semantically similar functions
can be implemented differently, run on different architectures, and
compiled with diverse compiler optimizations or obfuscations.
但是，此任务具有具有挑战性，因为语义类似的功能可以不同地实现，在不同的体系结构上运行，并以不同的编译器优化或混淆编译。

Most existing approaches match functions based on syntactic features
without understanding the functions’ execution semantics.
大多数现有方法匹配基于语法特征的函数，而无需了解函数的执行语义。

We present TREX, a transfer-learning-based framework, to automate
learning execution semantics explicitly from functions’ micro-traces
(a form of under-constrained dynamic traces) and transfer the learned
knowledge to match semantically similar functions.

我们呈现TREX，一种基于传输学习的框架，从函数的微迹（一种被限制的动态迹线的形式）明确地自动化学习执行语义，并传送学习知识以匹配语义上类似的函数。

While such micro-traces are known to be too imprecise to be directly
used to detect semantic similarity, our key insight is that these
traces can be used to teach an ML model the execution semantics of
different sequences of instructions.
虽然已知这种微迹情况直接用于检测语义相似性过于不精确，但我们的主要洞察力是这些迹线可用于指导ML模型不同指令序列的执行语义。

We thus design an unsupervised pretraining task, which trains the
model to learn execution semantics from the functions’ micro-traces
without any manual labeling or feature engineering effort.
因此，我们设计了一个无人监测的预测任务，该任务训练模型从函数的微迹中学习执行语义，而无需任何手动
标签或特征工程工作。

We then develop a novel neural architecture, hierarchical Transformer,
which can learn execution semantics from micro-traces during the
pretraining phase.
然后，我们开发一种新型神经结构，等级变压器，可以在预先预测阶段学习来自微迹的执行语义。

Finally, we finetune the pretrained model to match semantically
similar functions.
最后，我们微调预训练模型以匹配语义相似的函数。

We evaluate TREX on 1,472,066 function binaries from 13 popular
software projects.
我们对来自 13 个流行软件项目的 1,472,066 个函数二进制文件评估了 TREX。

These functions are from different architectures (x86, x64, ARM, and
MIPS) and compiled with 4 optimizations (O0-O3) and 5 obfuscations.
这些函数来自不同的架构（x86，x64，arm和mips），并用4个优化（o0-o3）和5个混淆编译。

TREX outperforms the state-of-the-art systems by 7.8%, 7.2%, and 14.3%
in crossarchitecture, optimization, and obfuscation function matching,
respectively, while running 8* faster.
TREX 在跨架构、优化和混淆函数匹配方面分别比最先进的系统高 7.8%、7.2% 和 14.3%，同时运行速度提高 8*。

Our ablation studies show that the pretraining task significantly
boosts the function matching performance, underscoring the importance
of learning execution semantics.
我们的消融研究表明，预先预订任务显着提高了匹配性能的功能，强调了学习执行语义的重要性。

Moreover, our extensive case studies demonstrate the practical use-
cases of TREX – on 180 real-world firmware images with their latest
version, TREX uncovers 16 vulnerabilities that have not been disclosed
by any previous studies.
此外，我们的广泛案例研究展示了TREX的实际用例-在180个现实世界固件图像中，TREX 发现了 16 个以前任何研究都没有披露过的漏洞。

We release the code and dataset of TREX at https:
//github.com/CUMLSec/trex.
我们在https：//github.com/cumlsec/trex中释放TREX的代码和数据集。

## 介绍

Semantic function similarity, which quantifies the behavioral
similarity between two functions, is a fundamental program analysis
capability with a broad spectrum of real-world security usages, such
as vulnerability detection [12], exploit generation [5], tracing
malware lineage [7], [41], and forensics [49].
语义功能相似度，这量化了两个函数之间的行为相似性，是具有广泛的实际安全用途的基本程序分析能力，例如漏洞检测[12]，利用生成[5]，跟踪恶意软件谱系[7]，[41]和取证[49]。

For example, OWASP lists “using components with known vulnerabilities”
as one of the top-10 application security risks in 2020 [56].
例如，OWASP列出“使用具有已知漏洞的组件”作为2020年的前10个应用程序安全风险之一[56]。

Therefore, identifying similar vulnerable functions in massive
software projects can save significant manual effort.
因此，在大规模软件项目中识别类似的易受攻击功能，可以节省重大的手动努力。

When matching semantically similar functions for securitycritical
applications (e.g., vulnerability discovery), we often have to deal
with software at binary level, such as commercial off-the-shelf
products (i.e., firmware images) and legacy programs.
在为安全关键应用程序（例如漏洞发现）匹配语义相似的功能时，我们通常必须处理二进制级别的软件，例如商业现成产品（即固件映像）和遗留程序。

However, this task is challenging, as the functions’ highlevel
information (e.g., data structure definitions) are removed during the
compilation process.
但是，此任务是具有挑战性的，因为在编译过程中删除了函数的函数高级信息（例如，数据结构定义）。

Establishing semantic similarity gets even harder when the functions
are compiled to run on different instruction set architectures with
various compiler optimizations or obfuscated with simple
transformations.
在编译函数以在不同的指令集架构上运行时，建立语义相似性甚至更加困难，其中包含各种编译器优化或使用简单的转换混淆。

Recently, Machine Learning (ML) based approaches have shown promise in tackling these challenges [25], [50], [77] by learning robust features that can identify similar function binaries across different architectures, compiler optimizations, or even some types of obfuscation.
最近，基于机器学习（ML）的方法在解决这些挑战[25]，[50]，[77]时，可以通过学习鲁棒功能来
解决这些挑战，可以识别不同架构，编译器优化，甚至某种类型的混淆的类似功能二进制文件

Specifically, ML models learn function representations (i.e., embeddings) from function binaries and use the distance between the embeddings of two functions to compute their similarity.
具体而言，ML模型从函数二进制文件学习功能表示（即嵌入），并使用两个函数的嵌入之间的距离来计算它们的相似度。

The smaller the distance, the more similar the functions are to each other.
距离越小，函数相似性越大.

Such approaches have achieved state-of-the-art results [25], [50],
[77], outperforming the traditional signature-based methods [79] using
hand-crafted features (e.g., number of basic blocks).
这种方法已经实现了最先进的结果[25]，[50]，[77]，优于使用手工制作的特征（例如，基本块数）的传统签名的方法[79]。

Such embedding distance-based strategy is particularly appealing for
large-scale function matching—taking only around 0.1 seconds searching
over one million functions [30].
这种嵌入距离的策略对于大规模函数匹配特别吸引力-仅在一百万个功能中搜索约0.1秒[30]。

Execution semantics.
执行语义。
Despite the impressive progress, it remains challenging for these
approaches to match semantically similar functions with disparate
syntax and structure [51].
尽管进展令人印象深刻，但这些方法仍然有挑战性，以匹配不同的语法和结构的语义类似的功能[51]。

An inherent cause is that the code semantics is characterized by its execution effects.
固有原因是代码语义的特征在于其执行效果。

However, all existing learning-based approaches are agnostic to program execution semantics, training only on the static code.
然而，所有现有的基于学习的方法都与程序执行语义无关，仅对静态代码进行训练。

Such a setting can easily lead a model into matching simple patterns, limiting their accuracy when such spurious patterns are absent or changed [1], [61].
这样的设置很容易导致模型匹配简单的模式，当这种虚假模式不存在或发生变化时，限制了它们的准确性 [1], [61]。

For instance, consider the following pair of x86 instrucations: mov
eax,2;lea ecx,[eax+4] are semantically equivalent to mov eax,2;lea
ecx,[eax+eax*2].
例如，考虑以下对X86的作用：MOVEAX，2;LEAECX，[EAX+4]是语义相当于MOVEAX
，2;LEAECX，[EAX+EAX*2]。

An ML model focusing on syntactic features might pick common
substrings (both sequences share the tokens mov, eax, lea, ecx) to
establish their similarity, which does not encode the key reason of
the semantic equivalence.
专注于句法特征的ML模型可能选择常见的子字符串（两个序列共享令牌MOV，EAX，LEA，ECX）以建立其相似性，这不会编码语义等效的关键原因。

Without grasping the approximate execution semantics, an ML model can
easily learn such spurious patterns without understanding the inherent
cause of the equivalence: [eax+eax*2] computes the same exact address
as [eax+4] when eax is 2.
在不掌握近似执行语义的情况下，ML 模型可以在不了解等价性的内在原因的情况下轻松学习此类虚假模式：当 eax 为 2 时，[eax+eax*2] 计算出与 [eax+4] 相同的确切地址。

Limitations of existing dynamic approaches.
现有动态方法的局限性。
Existing dynamic approaches try to avoid the issues described above by
directly comparing the dynamic behaviors of functions to determine
similarity.
现有的动态方法尝试通过直接比较功能的动态行为来避免上述问题来确定相似性。
As finding program inputs reaching the target functions is an
extremely challenging and timeconsuming task, the prior works perform
under-constrained dynamic execution by initializing the function input
states (eg, registers, memory) with random values and executing the
target functions directly [27]
由于寻找到达目标函数的程序输入是一项极具挑战性和耗时的任务，先前的工作通过用随机值初始化函数输入状态（例如，寄存器、内存）并直接执行目标函数来执行欠约束动态执行 [27]

Unfortunately, using such under-constrained execution traces directly
to compute function similarities often result in many false positives
[25].
不幸的是，直接使用这种约束不足的执行跟踪来计算函数相似性通常会导致许多误报 [25]。

For example, providing random inputs to two different functions with
strict input checks might always trigger similar shallow exception
handling codes and might look spuriously similar.
例如，向两个不同的具有严格的输入检查的函数提供随机输入可能总是会触发类似的浅层异常处理代码，并且可能看起来非常相似。

Our approach.
我们的方法。
This paper presents TREX (TRansfer-learning EXecution semantics) that trains ML models to learn the approximate execution semantics from under-constrained dynamic traces.
本文介绍了 TREX（转移学习执行语义），它训练 ML 模型以从欠约束动态轨迹中学习近似执行语义。

Unlike prior works, which use such traces to directly measure similarity, TREX pretrains the model on diverse traces to learn each instruction’s execution effect in its context.
与使用此类跟踪直接测量相似性的先前工作不同，TREX 在不同的跟踪上预训练模型，以了解每条指令在其上下文中的执行效果。

TREX then finetunes the model by transferring the learned knowledge from pretraining to match semantically similar functions (see Figure 1).
然后，TREX 通过将学习到的知识从预训练转移到匹配语义相似的函数来微调模型（见图 1）。

Our extensive experiments suggest that the approximately learned knowledge of execution semantics in pretraining significantly boosts the accuracy of matching semantically similar function binaries – TREX excels in matching functions from different architectures, optimizations, and obfuscations.
我们的大量实验表明，在预训练中近似学习的执行语义知识显着提高了匹配语义相似函数二进制文件的准确性——TREX 在匹配来自不同架构、优化和混淆的函数方面表现出色。

Our key observation is that while under-constrained dynamic execution traces tend to contain many infeasible states, they still encode precise execution effects of many individual instructions.
我们的主要观察是，虽然约束不足的动态执行跟踪往往包含许多不可行的状态，但它们仍然对许多单独指令的精确执行效果进行编码。

Thus, we can train an ML model to observe and learn the effect of different instructions present across a large number of underconstrained dynamic traces collected from diverse functions.
因此，我们可以训练一个 ML 模型来观察和学习从不同函数收集的大量欠约束动态轨迹中存在的不同指令的影响。

Once the model has gained an approximate understanding of execution semantics of various instructions, we can train it to match semantically similar functions by leveraging its learned knowledge.
一旦模型获得了对各种指示的执行语义的大致了解，我们就可以通过利用其学到的知识来训练它来匹配语义类似的函数。

As a result, during inference, we do not need to execute any functions on-the-fly while matching them [45], which saves significant runtime overhead.
因此，在推理过程中，我们不需要在匹配它们时即时执行任何函数 [45]，这节省了大量的运行时开销。

Moreover, our trained model does not need the under-constrained dynamic traces to match functions, it only uses the function instructions, but they are augmented with rich knowledge of execution semantics.
此外，我们的培训模型不需要受限制的动态迹线来匹配函数，它只使用函数指令，但它们是丰富的执行语义知识。

In this paper, we extend micro-execution [34], a form of under-constrained dynamic execution, to generate micro-traces of a function across multiple instruction set architectures.
在本文中，我们扩展了微执行[34]，一种被限制的动态执行的形式，以在多个指令集架构中生成函数的微迹。

A micro-trace consists of a sequence of aligned instructions and their corresponding program state values.
微跟踪包括一系列对齐指令及其相应的程序状态值。

We pretrain the model on a large number of micro-traces gathered from diverse functions as part of training data using the masked language modeling (masked LM) task.
我们使用掩码语言建模 (masked LM) 任务在从不同功能收集的大量微轨迹上预训练模型，作为训练数据的一部分。

Notably, masked LM masks random parts in the sequence and asks the model to predict masked parts based on their context.
值得注意的是，掩码 LM 掩码序列中的随机部分，并要求模型根据上下文预测掩码部分。

This design forces the model to learn approximately how a function executes to correctly infer the missing values, which automates learning execution semantics without manual feature engineering.
这种设计迫使模型大致了解函数如何执行以正确推断缺失值，从而无需手动特征工程即可自动学习执行语义。

Masked LM is also fully self-supervised [22] – TREX can thus be trained and further improved with arbitrary functions found in the wild.
Masked LM 也是完全自我监督的 [22] – 因此可以使用在野外发现的任意函数来训练和进一步改进 TREX。

To this end, we design a hierarchical Transformer [75] that supports learning approximate execution semantics.
为此，我们设计了一个支持学习近似执行语义的分层变换器[75]。
Specifically, our architecture models micro-trace values explicitly.
具体而言，我们的体系结构明确模拟微跟踪值。

By contrast, existing approaches often treat the numerical values as a dummy token [25], [50] to avoid prohibitively large vocabulary size, which cannot effectively learn the rich dependencies between concrete values that likely encode key function semantics.
相比之下，现有方法通常将数值视为伪令牌[25]，[50]以避免过大的词汇量，这不能有效地学习可能对关键函数语义的具体值之间的丰富依赖性。
Moreover, our architecture’s self-attention layer is designed to model long-range dependencies in a sequence [75] efficiently.
此外，我们的架构的自我注意层设计用于序列中的长距离依赖性[75]有效。
Therefore, TREX can support roughly 170 longer sequence and runs 8 faster than existing neural architectures, essential to learning embeddings of long function execution traces.
因此，TREX可以高出大约170个更长的序列，并且比现有的神经架构快8次，对于长函数执行迹线的学习嵌入至关重要。

We evaluate TREX on 1,472,066 functions collected from 13 popular open-source software projects across 4 architectures (x86, x64, ARM, and MIPS) and compiled with 4 optimizations (O0-O3), and 5 obfuscation strategies [78].
我们评估了从4架构（X86，X64，ARM和MIPS）的13个流行的开源软件项目收集的1,472,066函数上的TREX，并用4个优化（O0-O3）和5个混淆策略编译[78]。
TREX outperforms the state-of-the-art systems by 7.8%, 7.2%, and 14.3% in matching functions across different architectures, optimizations, and obfuscations, respectively.
TREX分别优于最先进的系统，分别在不同架构，优化和混淆的匹配功能中以7.8％，7.2％和14.3％。
Our ablation studies show that the pretraining task significantly improves the accuracy of matching semantically similar functions (by 15.7%).
我们的消融研究表明，预先预订任务显着提高了语义类似功能的准确性（15.7％）。
We also apply TREX in searching vulnerable functions in 180 realworld firmware images developed by well-known vendors and deployed in diverse embedded systems, including WLAN routers, smart cameras, and solar panels.
我们还将TREX应用于由众所周知的供应商开发的180个RealWorld固件图像中搜索弱势职能，并在不同的嵌入式系统中部署，包括WLAN路由器，智能摄像机和太阳能电池板。
Our case study shows that TREX helps find 16 CVEs in these firmware images, which have not been disclosed in previous studies.
我们的案例研究表明，TREX有助于在这些固件图像中找到16个CVVE，该图像尚未在以前的研究中披露。
We make the following contributions.
我们提出以下贡献。

We propose a new approach to matching semantically similar functions: we first train the model to learn approximate program execution semantics from micro-traces, a form of under-constrained dynamic traces, and then transfer the learned knowledge to identify semantically similar functions.
我们提出了一种匹配语义相似函数的新方法：我们首先训练模型从微轨迹中学习近似程序执行语义，这是一种欠约束动态轨迹，然后转移学习到的知识来识别语义相似的函数。

We extend micro-execution to support different architectures to collect micro-traces for training.
我们扩展了微执行以支持不同的架构来收集用于训练的微跟踪。

We then develop a novel neural architecture – hierarchical Transformer – to learn approximate execution semantics from micro-traces.
然后，我们开发了一种新颖的神经架构——分层 Transformer——以从微迹中学习近似执行语义。

We implement TREX and evaluate it on 1,472,066 functions from 13 popular software projects and libraries.
我们在13个流行的软件项目和库中实现TREX并评估1,472,066个功能。

TREX outperforms the state-of-the-art tools by 7.8%, 7%, and 14.3%, in cross-architecture, optimization, and obfuscation function matching, respectively, while running up to 8 faster.
在跨架构、优化和混淆函数匹配方面，TREX 的性能分别比最先进的工具高 7.8%、7% 和 14.3%，同时运行速度提高了 8 倍。

Moreover, TREX helps uncover 16 vulnerabilities in 180 real-world firmware images with the latest version that are not disclosed by previous studies.
此外，TREX 帮助发现了 180 个现实世界固件映像中的 16 个漏洞，这些漏洞具有以前的研究未披露的最新版本。

We release the code and dataset of TREX at https://github.com/CUMLSec/trex.
我们在https://github.com/cumlsec/trex中发布TREX的代码和数据集。

## 概述

In this section, we use the real-world functions as motivating examples to describe the challenges of matching semantically similar functions.
在本节中，我们使用现实世界的函数作为激励示例来描述匹配语义相似函数的挑战。
We then overview our approach, focusing on how our pretraining task (masked LM) addresses the challenges.
然后，我们概述了我们的方法，重点关注我们的预训练任务（masked LM）如何应对挑战。

### 挑战性案件

We use three semantically equivalent but syntactically different function pairs to demonstrate some challenges of learning from only static code.
我们使用三个语义等效但语法不同的函数对来演示仅从静态代码中学习的一些挑战。
Figure 2 shows the (partial) assembly code of each function.
图2显示了每个函数的（部分）汇编代码。

Cross-architecture example.
跨架构示例。
Consider the functions in Figure 2a.
考虑图2a中的函数。
Two functions have the same execution semantics as both functions take the lower 12-bit of a register and compare it to 0x80.
两个函数具有相同的执行语义，因为两个函数都取寄存器的低 12 位并将其与 0x80 进行比较。
Detecting this similarity requires understanding the approximate execution semantics of and in x86 and lsl/lsr in ARM.
检测此相似度需要了解X86和LSL/LSR中的近似执行语义。
Moreover, it also requires understanding how the values (i.e., 0xfff and 0x14) in the code are manipulated.
此外，它还需要了解如何操纵代码中的值（即0xFFF和0x14）。

However, all existing ML-based approaches [50] only learn on static code without observing each instruction’s real execution effect.
然而，所有现有的基于机器学习的方法 [50] 只学习静态代码，而没有观察每条指令的实际执行效果。

Furthermore, to mitigate the potentially prohibitive vocabulary size (i.e., all possible memory addresses), existing approaches replace all register values and memory addresses with an abstract dummy symbol [26], [50].
此外，为了减轻潜在的令人望而却步的词汇量（即所有可能的内存地址），现有方法将所有寄存器值和内存地址替换为抽象的虚拟符号 [26]、[50]。

They thus cannot access the specific byte values to determine inherent similarity.
因此，它们无法访问特定的字节值以确定固有的相似性。

Cross-optimization example.
交叉优化示例。
Now consider two functions in Figure 2b.
现在考虑图2B中的两个功能。
They are semantically equivalent as [ebp+8] and [esp+4] access the same memory location, i.e., the function’s first argument pushed on the stack by the caller.
它们是语义上等同于[EBP+8]和[ESP+4]访问相同的内存位置，即函数由呼叫者推出堆栈上的第一个参数。
To detect such similarity, the model should understand push decreases the stack pointer esp by 4. The model should also notice that mov at line 2 assigns the decremented esp to ebp such that ebp+8 in the upper function equals esp+4 in the lower
要检测到这样的相似性，模型应该理解推送堆栈指针ESP到4.该模型还应注意到第2行的MOV将递减的ESP递减为eBP，使得上函数中的EBP+8等于较低的ESP+4
function.
功能。
However, such dynamic information is not reflected in the static code.
然而，这种动态信息没有反映在静态代码中。

Cross-obfuscation example.
交叉混淆示例。
Figure 2c demonstrates a simple obfuscation by instruction substitution, which essentially replaces eax+1 with eax-(-1).
图2c通过指令替代说明了简单的混淆，基本上用EAX-（-1）替换EAX+1。
Detecting the equivalence requires understanding approximately how arithmetic operations such as xor, sub, and add, executes.
检测等同物需要了解Xor，Sub和Add等算术运算如何执行。
However, static information is not enough to expose such knowledge.
但是，静态信息不足以暴露这些知识。

### 在微迹上预训练掩码 LM

This section describes how the pretraining task, masked LM, on functions’ micro-traces encourages the model to learn execution semantics.
本节介绍函数微跟踪上的预训练任务（masked LM）如何鼓励模型学习执行语义。

Although it remains an open research question to explicitly prove certain knowledge is encoded by such language modeling task [70], we focus on describing the intuition behind the masked LM – why predicting masked codes and values in micro-traces can help address the challenging cases in Figure 2.
尽管明确证明某些知识是由此类语言建模任务编码的仍然是一个开放的研究问题 [70]，但我们专注于描述掩码 LM 背后的直觉——为什么预测微迹中的掩码代码和值可以帮助解决图 2 中具有挑战性的案例。

Masked LM.
蒙面LM。
Recall the operation of masked LM: given a function’s micro-trace (i.e., values and instructions), we mask some random parts and train the model to predict the masked parts using those not masked.
回想屏蔽 LM 的操作：给定一个函数的微跟踪（即值和指令），我们屏蔽一些随机部分并训练模型使用未屏蔽的部分来预测被屏蔽的部分。

Note that pretraining with masked LM does not need any manual labeling effort, as it only predicts the masked part in the input micro-traces without any additional labeling effort.
请注意，使用掩码 LM 进行预训练不需要任何手动标记工作，因为它仅预测输入微迹中的掩码部分，而无需任何额外的标记工作。

Therefore, TREX can be trained and further improved with a substantial number of functions found in the wild.
因此，可以通过在野外发现的大量功能进行培训并进一步改善Trex。
The benefit of this is that a certain instruction not micro-executed in one function is highly likely to appear in at least one of the other functions’ micro-traces, supporting TREX to approximate diverse instructions’ execution semantics.
这样做的好处是，在一个功能中没有微型执行的某个指令很可能出现在其他功能的微迹中的至少一个中，支持TREX以近似不同的指令执行语义。

Masking register.
掩蔽寄存器。
Consider the functions in Figure 2c, where they essentially increment the value at stack location [rbp-0x2c] by 1. The upper function directly loads the value to eax, increments by 1, and stores the value in eax back to stack.
考虑图2c中的函数，其中它们基本上将堆栈位置[RBP-0x2C]的值递增1.上函数直接将值加载到EAx，增量为1，并将EAX中的值存储回堆栈。
The lower function, by contrast, takes a convoluted way by first letting ecx to hold the value -1, and decrements eax by ecx, and stores the value in eax back to stack.
相比之下，较低的函数通过首先让ECX保持valy-1，并通过ECX递减EAX，并将EAX的值存储回堆叠。

We mask the eax at line 3 in the upper function.
我们在上部功能中将EAX掩盖。
We find that our pretrained model can correctly predict its name and dynamic value.
我们发现我们的预制模型可以正确预测其名称和动态值。
This implies the model understands the semantics of add and can deduce the value of eax in line 3 after observing the value of eax in line 2 (before the addition takes the effect).
这意味着模型理解添加的语义，并且可以在观察第2行中的EAX的值后在第3行中推断出EAx的值（在添加之前效果之前）。
We also find the model can recover the values of masked ecx in line 4 and eax in line 5, implying the model understands the execution effect of xor and sub.
我们还发现模型可以在第5行中恢复蒙版ECX的值，暗示模型了解XOR和SUB的执行效果。

The understanding of such semantics can significantly improve the robustness in matching similar functions – when finetuned to match similar functions, the model is more likely to learn to attribute the similarity to their similar execution effects, instead of their syntactic similarity.
对这种语义的理解可以显着提高匹配类似函数的鲁棒性-当FineTuned匹配类似的函数时，模型更有可能学会将相似性归因于它们类似的执行效果，而不是它们的语法相似性。

Masking opcode.
屏蔽操作码。
Besides masking the register and its value, we can also mask the opcode of an instruction.
除了屏蔽寄存器及其值外，我们还可以掩盖指令的操作码。
Predicting the opcode requires the model to understand the execution effect of each opcode.
预测操作码需要模型来理解每个操作码的执行效果。
Consider Figure 2b, where we mask mov in line 2 of upper function.
考虑图2B，其中我们在上函数的第2行中掩盖MOV。
We find our pretrained model predicts mov with the largest probability (larger than the other potential candidates such as add, inc, etc.).
我们发现我们的预制模型预测了具有最大概率的MOV（比其他潜在候选者，如Add，Inc等）。

To correctly predict the opcode, the model should have learned several key aspects of the function semantics.
要正确预测操作码，该模型应该学习了函数语义的几个关键方面。
First, according to its context, i.e., the value of ebp at line 3 and esp at line 2, it learns mov is most probable as it assigns the value of esp to ebp.
首先，根据其上下文，即，第3行的EBP值和ESP在第2行，它学习MOV是最可能的，因为它将ESP的值分配给EBP。
Other opcodes are less likely as their execution effect conflicts with the observed resulting register values.
其他操作频率不太可能与观察到的结果寄存器值相冲突。
This also implicitly implies the model learns the approximate execution semantics of mov.
这也隐含地意味着模型了解MOV的近似执行语义。
Second, the model also learns the common calling convention and basic syntax of x86 instructions, e.g., only a subset of opcodes accept two operands (ebp,esp).
其次，该模型还了解X86指令的常见调用约定和基本语法，例如，仅接受两个操作数（EBP，ESP）的Opcodes的子集。
It can thus exclude many syntactically impossible opcodes such as push, jmp, etc.
因此，它可以排除许多句法不可能的操作码，例如推，JMP等。

The model can thus infer ebp (line 3 of upper function) equals to esp.
因此，该模型可以推断出eBP（上函数的第3行）等于ESP。
The model may have also learned push decrements stack pointer esp by 4 bytes, from other masked samples.
该模型还可以从其他掩码样本中学习将推倍堆叠指针ESP递推减少4个字节。
Therefore, when the pretrained model is finetuned to match the two functions, the model is more likely to learn that the semantic equivalence is due to that [ebp+8] in the upper function and [esp+4] in the lower function refer to
因此，当预先训练的模型是FINETUNED以匹配两个功能时，该模型更有可能知道语义等效是由于下函数中的[EBP+8]和较低函数中的[ESP+4]引用
the same address, instead of their similar syntax.
相同的地址，而不是它们类似的语法。

Other masking strategies.
其他掩蔽策略。
Note that we are not constrained by the number or the type of items (ie, register, opcode, etc.) in the instructions to mask, ie, we can mask complete instructions or even a consecutive sequence of instructions, and we can mask dynamic
请注意，我们不会受到掩码的指令中的数量或项目类型或类型（即，寄存器，操作码等）的限制，即，我们可以屏蔽完整的指令甚至连续指令序列，我们可以掩盖动态
values of random instructions' inputoutput.
随机指令输入输出的值。

Moreover, the masking operation dynamically selects random subsets of code blocks and program states at each training iteration and on different training samples.
此外，掩蔽操作在每个训练迭代和不同训练样本上动态地选择代码块和程序状态的随机子集。
As a result, it enables the model to learn the diverse and composite effect of the instruction sequence, essential to detecting similarity between functions with various instructions.
结果，它使模型能够学习指令序列的多样化和复合效果，对于检测具有各种指令之间的功能之间的相似性必不可少。
In this paper, we adopt a completely randomized strategy to choose what part of the micro-trace to mask with a fixed masking percentage (see Section IV-C for details).
在本文中，我们采用完全随机的策略来选择微量痕迹的哪个部分以固定的掩蔽百分比（参见IV-C部分详情）。
However, we envision a quite interesting future work to study a better (but still cheap) strategy to dynamically choose where and how much to mask.
但是，我们设想了一个非常有趣的未来工作，以研究更好（但仍然便宜）战略，以动态选择掩盖的地方和多少。

III.
III。
THREAT MODEL.
威胁模型。

We assume no access to the debug symbols or source while comparing binaries.
在比较二进制文件时，我们假设无法访问调试符号或源。
Indeed, there exist many approaches to reconstruct functions from stripped binaries [4], [6], [24], [62], [72].
实际上，存在许多方法可以从剥离二进制文件[4]，[6]，[24]，[62]，[72]。
Moreover, we assume the binary can be readily disassembled, i.e., it is not packed nor transformed by virtualizationbased obfuscator [73], [74].
此外，我们假设二进制可以容易地拆卸，即，它没有被虚拟化的混淆器[73]，[74]填充也不转换。

Semantic similarity.
语义相似度。
We consider two semantically similar functions as having the same input-output behavior (i.e., given the same input, two functions produce the same output).
我们考虑具有相同输入输出行为的两个语义类似的功能（即，给定相同的输入，两个函数产生相同的输出）。
Similar to previous works [25], [50], [77], we treat functions compiled from the same source as similar, regardless of architectures, compilers, optimizations, and obfuscation transforms.
与以前的作品类似[25]，[50]，[77]，我们将从与相同的源编译的函数和类似的源代码，无论体系结构，编译器，优化和混淆转换如何。

IV.
IV。
METHODOLOGY.
方法。

This section describes TREX’s design specifics, including our micro-tracing semantics, our learning architecture’s details, and pretraining and finetuning workflow.
本节介绍了TREX的设计细节，包括我们的微跟踪语义，我们的学习架构的细节和预先训练和FineTuning工作流程。

A. Micro-tracing Semantics.
A.微跟踪语义。

We implement micro-execution by Godefroid [34] to handle x64, ARM, and MIPS, where the original paper only describes x86 as the use case.
我们通过Godefroid[34]实现微型执行来处理X64，ARM和MIPS，原始纸张仅将X86描述为用例。
In the following, we briefly explain how we micro-execute an individual function binary, highlighting the key algorithms in handling different types of instructions.
在下文中，我们简要解释了我们如何微型执行单独的功能二进制文件，突出显示处理不同类型指令的关键算法。

IR Language.
红外语言。
To abstract away the complexity of different architectures’ assembly syntax, we introduce a low-level intermediate representation (IR) that models function assembly code.
要抽出不同架构的装配语法的复杂性，我们引入了模型函数汇编代码的低级中间表示（IR）。
We only include a subset of the language specifics to illustrate the implementation algorithm.
我们仅包括语言细节的子集来说明实现算法。
Figure 3 shows the grammar of the IR.
图3显示了IR的语法。
Note that the IR here only serves to facilitate the discussion of our micro-tracing implementation.
请注意，IR在此仅用于促进我们微跟踪实现的讨论。

In our implementation, we use real assembly instructions and tokenize them as model’s input (Section IV-B).
在我们的实施中，我们使用真正的装配说明并将其授予为模型的输入（部分IV-B）。

Notably, we denote memory reads and writes by load(e) and store(ev; ea) (ie, store the value expression ev to address expression ea), which generalize from both the loadstore architecture (ie, ARM, MIPS) and register
值得注意的是，我们表示通过加载（e）和存储（ep;ea）（即，将值表达式EV存储到地址表达式EA）的内存读取和写入，这概括了LoadStore架构（即，ARM，MIP）和寄存器
-memory architecture (ie, x86).
-制造架构（即，x86）。
Both operations can take as input e – an expression that can be an explicit hexadecimal number (denoting the address or a constant), a register, or a result of an operation on two registers.
这两个操作都可以作为输入e-一种表达式，可以是两个寄存器上的显式十六进制数（表示地址或常数），寄存器或操作的结果。
We use jmp to denote the general jump instruction, which can be both direct or indirect jump (i.e., the expression ea can be a constant c or a register r).
我们使用JMP表示一般跳转指令，它可以是直接或间接跳转（即，表达式EA可以是常数C或寄存器R）。
The jump instruction can also be unconditional or conditional.
跳转指令也可以是无条件的或有条件的。
Therefore, the first parameter in jmp is the conditional expression ec and unconditional jump will set ec to true.
因此，JMP中的第一个参数是条件表达式EC，无条件跳转将EC设置为TRUE。
We represent function invocations and returns by call and ret, where call is parameterized by an expression, which can be an address (direct call) or a register (indirect call).
我们代表函数调用并通过呼叫和返回返回，呼叫由表达式参数化，可以是地址（直接呼叫）或寄存器（间接呼叫）。

Micro-tracing algorithm.
微跟踪算法。
Algorithm 1 outlines the basic steps of micro-tracing a given function f.
算法1概述了给定函数f的微跟踪的基本步骤。
First, it initializes the memory to load the code and the corresponding stack.
首先，它初始化内存以加载代码和相应的堆栈。
It then initializes all registers except the special-purpose register, such as the stack pointer or the program counter.
然后，它初始化除专用寄存器之外的所有寄存器，例如堆栈指针或程序计数器。
Then it starts linearly executing instructions of f.
然后它开始线性执行F的指令。
We map the memory address on-demand if the instruction access the memory (i.e., read/write).
如果指令访问存储器（即，读/写），我们会按需映射内存地址。
If the instruction reads from memory, we further initialize a random value in the specific memory addresses.
如果指令从内存中读取，则进一步初始化特定内存地址中的随机值。
For call/jump instructions, we first examine the target address and skip the invalid jump/call, known as “forced execution” [63].
对于呼叫/跳转指令，我们首先检查目标地址并跳过无效的跳转/呼叫，称为“强制执行”[63]。
By skipping unreachable jumps and calls, it can keep executing the function till the end of the function and exposes more behaviors, e.g., skipping potential input check exceptions.
通过跳过无法访问的跳转和呼叫，它可以继续执行功能直到函数的末尾并暴露更多行为，例如，跳过潜在输入检查异常。
Since the nop instructions can serve as padding between instructions within a function, we simply skip nop.
由于NOP指令可以作为函数内的指令之间的填充，因此我们只需跳过NOP。
We terminate the micro-tracing when it finishes executing all instructions, reaches ret, or times out.
当它完成执行所有指令时，我们终止微跟踪，达到RET或超时。
Figure 13 and 14 demonstrate sample micro-traces of real-world functions.
图13和14展示了现实世界的样本微迹。

B. Input Representation.
B.输入表示。

Formally, given a function f (i.e., assembly code) and its micro-trace t (by micro-executing f), we prepare the model input x, consisting of 5 types of token sequence with the same size n.
正式地，给定函数f（即，汇编代码）及其微跟踪t（通过微跟踪t），我们准备模型输入x，由具有相同大小的5种类型的令牌序列组成。
Figure 4 shows the model input example and how they are masked and processed by the hierarchical Transformer to predict the corresponding output as a pretraining task.
图4显示了模型输入示例以及它们是由分层变换器屏蔽和处理的模型输入示例，以将相应的输出预测为预先训练任务。

Micro-trace code sequence.
微跟踪码序列。
The first sequence xf is the assembly code sequence: xf = fmov;
第一个序列XF是汇编代码序列：XF=FMOV;
eax;+;
eax;+;
:::gn, generated by tokenizing the assembly instructions in the micro-trace.
:::GN，通过授权微跟踪中的装配说明生成。
We treat all symbols appear in the assembly instructions as tokens.
我们将所有符号视为令牌的装配说明中显示所有符号。

Such a tokenization aims to preserve the critical hint of the syntax and semantics of the assembly instructions.
这种令牌化旨在保留大量说明的语法和语义的临界提示。
For example, we consider even punctuation to be one of the tokens, eg, “,”, “[”, “]”, as “,” implies the token before and after it as destination and source of mov (in Intel syntax)
例如，我们认为即使是标点符号也是一个令牌，例如“，”，“[”，“，”，“，”，“”，“在它之前和之后暗示的令牌作为目标和源（在英特尔语法中）
, respectively, and “[” and “]” denote taking the address of the operands reside in between them.
分别和“[”和“]”表示，拍摄操作数的地址驻留在它们之间。

We take special treatment of numerical values appear in the assembly code.
我们采取特殊处理数值出现在汇编代码中。
Treating numerical values as regular text tokens can incur prohibitively large vocabulary size, e.g., 232 number of possibilities on 32-bit architectures.
将数值视为常规文本令牌可能会产生过大的词汇量，例如32位架构上的232个可能性。
To avoid this problem, we move all numeric values to the micro-trace value sequence (that will be learned by an additional neural network as detailed in the following) and replace them with a special token num (eg, last token of input in Figure
为了避免这个问题，我们将所有数值移动到微跟踪值序列（将通过以下内容详述的附加神经网络学习），并用特殊的令牌Num替换它们（例如，图中输入的最后令牌
4).
4）。
With all these preprocessing steps, the vocabulary size of xf across all architectures is 3,300.
通过所有这些预处理步骤，所有架构的XF的词汇量为3,300。

Micro-trace value sequence.
微量痕量值序列。
The second sequence xt is the micro-trace value sequence, where each token in xt is the dynamic value from micro-tracing the corresponding code.
第二个序列XT是微跟踪值序列，其中XT中的每个令牌是从微跟踪相应代码的动态值。
As discussed in Section II, we keep explicit values (instead of a dummy value used by existing approaches) in xt.
如第II部分所讨论的，我们在XT中保留显式值（而不是现有方法使用的虚拟值）。
Notably, we use the dynamic value for each token (e.g., register) in an instruction before it is executed.
值得注意的是，我们在执行之前的指令中使用每个令牌（例如，寄存器）的动态值。
For example, in mov eax,0x8;
例如，在MOVEAX，0x8;
mov eax,0x3, the dynamic value of the second eax is 0x8, as we take the value of eax before mov eax,0x3 is executed.
MOVEAX，0x3，第二EAX的动态值为0x8，因为我们在MOVEAX之前取得EAX的值，执行0x3。
For code token without dynamic value, e.g., mov, we use dummy values (see below).
对于没有动态值的代码令牌，例如，MOV，我们使用虚拟值（见下文）。

Position sequences.
位置序列。
The position of each code and value token is critical for inferring binary semantics.
每个代码和值令牌的位置对于推断二进制语义至关重要。
Unlike natural language, where swapping two words can roughly preserve the same semantic meaning, swapping two operands can significantly change the instructions.
与自然语言不同，在交换两个单词可以大致保持相同的语义含义，交换两个操作数可以显着改变指令。
To encode the inductive bias of position into our model, we introduce instruction position sequence xc and opcode/operand position sequence xo to represent the relative positions between the instructions and within each instruction.
要将位置的归纳偏差进行编码到我们的模型中，我们介绍了指令位置序列XC和操作码/操作数位置序列XO，以表示指令和每个指令内的相对位置。
As shown in Figure 4, xc is a sequence of integers encoding the position of each instruction.
如图4所示，XC是编码每个指令位置的整数序列。

All opcodes/operands within a single instruction share the same value.
单个指令中的所有操作码/操作数共享相同的值。
xo is a sequence of integers encoding the position of each opcode and operands within a single instruction.
XO是一系列整数，编码每个操作码和操作数在单个指令中的位置。

Architecture sequence.
架构序列。
Finally, we feed the model with an extra sequence xa, describing the input binary’s instruction set architecture.
最后，我们用额外的序列XA馈送模型，描述了输入二进制指令集架构。
The vocabulary of xa consists of 4 architectures: xa = fx86, x64, ARM, MIPSgn.
XA的词汇包括4个架构：XA=FX86，X64，ARM，MIPSGN。
This setting helps the model to distinguish between the syntax of different architecture.
此设置有助于模型区分不同架构的语法。

Encoding numeric values.
编码数值。
As mentioned above, treating concrete values as independent tokens can lead to prohibitively large vocabulary size.
如上所述，将混凝土值视为独立的令牌，可以导致过大的词汇量。
We design a hierarchical input encoding scheme to address this challenge.
我们设计一个分层输入编码方案来解决这一挑战。
Specifically, let xti denote the i-th value in xt.
具体来说，让XTI表示XT中的第i值。
We represent xti as an (padded) 8-byte fixed-length byte sequence xti =f0x00, ..., 0xffg8 ordered in Big-Endian.
我们将XTI代表为（填充）8字节的固定长度字节序列XTI=F0x00，...，0xFFG8在Big-Endian中排序。
We then feed xti to a 2-layer bidirectional LSTM (bi-LSTM) and take its last hidden cell’s embedding as the value representation ti = bi-LSTM(xti ).
然后，我们将XTI馈送到2层双向LSTM（Bi-LSTM），并将其最后一个隐藏的单元格嵌入为值表示Ti=Bi-LSTM（XTI）。
Here ti denotes the output of applying the embedding to xti .
这里TI表示将嵌入到XTI应用于XTI的输出。
To make the micro-trace code tokens without dynamic values (e.g., opcode) align with the byte sequence, we use a dummy sequence (##) with the same length.
为了使微跟踪代码令牌没有动态值（例如，操作码）与字节序列对齐，我们使用具有相同长度的虚拟序列（##）。
Figure 4 (right-hand side) illustrates how bi-LSTM takes the byte sequence and computes the embedding.
图4（右侧）示出了双LSTM如何采用字节序列并计算嵌入。

Such a design significantly reduces the vocabulary size as now we have only 256 possible byte values to encode.
这样的设计显着降低了现在的词汇量，我们只有256个可能的字节值来编码。
Moreover, bi-LSTM encodes the potential dependencies between high and low bytes within a single value.
此外，Bi-LSTM在单个值内对高低字节之间的潜在依赖性进行编码。
This setting thus supports learning better relationships between different dynamic values (e.g., memory region and offset) as opposed to treating all values as dummy tokens [71].
因此，该设置支持在不同动态值（例如，存储区域和偏移）之间的学习更好的关系，而不是将所有值视为虚拟令牌[71]。
